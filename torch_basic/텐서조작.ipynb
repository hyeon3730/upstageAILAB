{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKjg766IvMdI"
   },
   "source": [
    "# 텐서 조작 (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXUZV0j3nBJd"
   },
   "source": [
    "## 실습 개요\n",
    "\n",
    "1) **실습 목적**\n",
    "\n",
    "\n",
    "이번 실습은 이론으로 배웠던 **PyTorch의 조작법**을 코드를 통해 확인하고 스스로 실행해 볼 수 있도록 구성하였습니다. 여러 가지 조작 **예시**들과 그에 따른 **결과**를 통해 PyTorch의 조작법을 쉽게 익힐 수 있습니다.😊\n",
    "\n",
    "\n",
    "2) **수강 목표**\n",
    "\n",
    "- 텐서를 생성하고 다양한 타입을 텐서로 변환할 수 있다. (<font color=red><b>Generating Tensor</b></font>)\n",
    "- 텐서의 모양을 변경하고 구현할 수 있다. (<font color =red><b>Reshaping Tensor</b></font>)\n",
    "- 텐서를 합치거나 나누거나 할 수 있다. (<font color=red><b>Merging and Spliting Tensor</font></b>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5CYPnoeDoRu"
   },
   "source": [
    "### 실습 목차\n",
    "* 1. 텐서 이해하기\n",
    "  * 1-1. 텐서를 생성하고 텐서로 변환하는 방법을 이해 및 실습\n",
    "  * 1-2. 텐서에서의 indexing 이해 및 실습\n",
    "* 2. 텐서의 모양 바꾸기\n",
    "  * 2-1. 텐서의 shape 을 바꾸는 여러가지 함수 이해 및 실습\n",
    "  * 2-2. 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "  * 2-3. 역할이 비슷한 함수들의 차이 이해 및 실습\n",
    "* 3. 텐서 합치기와 나누기\n",
    "  * 3-1. 여러 텐서를 합치는 방법에 대한 이해 및 실습\n",
    "  * 3-2. 하나의 텐서를 여러개로 나누는 방법에 대한 이해 및 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMbQ5FwAX2Bl"
   },
   "source": [
    "### 환경 설정\n",
    "> PyTorch 설치 및 불러오기\n",
    "\n",
    "<font color = blue><b>\n",
    "- 패키지 설치 및 임포트\n",
    "</font><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6018,
     "status": "ok",
     "timestamp": 1689144160021,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "mAhF-zKFUXqB",
    "outputId": "74cf0291-19e0-4670-943f-3837b49df0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.1\n",
      "  Obtaining dependency information for torch==2.0.1 from https://files.pythonhosted.org/packages/8c/4d/17e07377c9c3d1a0c4eb3fde1c7c16b5a0ce6133ddbabc08ceef6b7f2645/torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-cuda-nvrtc-cu11==11.7.99 from https://files.pythonhosted.org/packages/ef/25/922c5996aada6611b79b53985af7999fc629aee1d5d001b6a22431e18fec/nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-cuda-runtime-cu11==11.7.99 from https://files.pythonhosted.org/packages/36/92/89cf558b514125d2ebd8344dd2f0533404b416486ff681d5434a5832a019/nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-cuda-cupti-cu11==11.7.101 from https://files.pythonhosted.org/packages/e6/9d/dd0cdcd800e642e3c82ee3b5987c751afd4f3fb9cc2752517f42c3bc6e49/nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu11==8.5.0.96 from https://files.pythonhosted.org/packages/dc/30/66d4347d6e864334da5bb1c7571305e501dcb11b9155971421bb7bb5315f/nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-cublas-cu11==11.10.3.66 from https://files.pythonhosted.org/packages/ce/41/fdeb62b5437996e841d83d7d2714ca75b886547ee8017ee2fe6ea409d983/nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-cufft-cu11==10.9.0.58 from https://files.pythonhosted.org/packages/64/c8/133717b43182ba063803e983e7680a94826a9f4ff5734af0ca315803f1b3/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-curand-cu11==10.2.10.91 from https://files.pythonhosted.org/packages/8f/11/af78d54b2420e64a4dd19e704f5bb69dcb5a6a3138b4465d6a48cdf59a21/nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-cusolver-cu11==11.4.0.1 from https://files.pythonhosted.org/packages/3e/77/66149e3153b19312fb782ea367f3f950123b93916a45538b573fe373570a/nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-cusparse-cu11==11.7.4.91 from https://files.pythonhosted.org/packages/ea/6f/6d032cc1bb7db88a989ddce3f4968419a7edeafda362847f42f614b1f845/nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-nccl-cu11==2.14.3 from https://files.pythonhosted.org/packages/55/92/914cdb650b6a5d1478f83148597a25e90ea37d739bd563c5096b0e8a5f43/nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
      "  Obtaining dependency information for nvidia-nvtx-cu11==11.7.91 from https://files.pythonhosted.org/packages/23/d5/09493ff0e64fd77523afbbb075108f27a13790479efe86b9ffb4587671b5/nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1)\n",
      "  Obtaining dependency information for triton==2.0.0 from https://files.pythonhosted.org/packages/ca/31/ff6be541195daf77aa5c72303b2354661a69e717967d44d91eb4f3fdce32/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (68.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.41.2)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1)\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/b8/8d/a7cf7573df4c67de7cb4b2e36cfc790462b90813baa14604c3693a23d89b/cmake-3.31.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading cmake-3.31.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
      "  Obtaining dependency information for lit from https://files.pythonhosted.org/packages/96/06/b36f150fa7c5bcc96a31a4d19a20fddbd1d965b6f02510b57a3bb8d4b930/lit-18.1.8-py3-none-any.whl.metadata\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.31.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, cmake, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0\n",
      "    Uninstalling torch-2.1.0:\n",
      "      Successfully uninstalled torch-2.1.0\n",
      "Successfully installed cmake-3.31.1 lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==2.0.1 # PyTorch 를 가장 최근 버전으로 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OW0tn1H4WVQA"
   },
   "outputs": [],
   "source": [
    "import torch # PyTorch 불러오기\n",
    "import numpy as np # numpy 불러오기\n",
    "import warnings # 경고 문구 제거\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siW2vxW_5R8u"
   },
   "source": [
    "## 1. 텐서 이해하기\n",
    "\n",
    "```\n",
    "💡 목차 개요 : 텐서의 생성과 텐서로 변환하는 과정을 이해하고, 값을 변환하거나 추출할 수 있는 방법을 알아봅니다.\n",
    "```\n",
    "\n",
    "- 1-1. 텐서를 생성하고 텐서로 변환하는 방법을 이해 및 실습\n",
    "- 1-2. 텐서에서의 indexing 이해 및 실습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhHH9j78vg0"
   },
   "source": [
    "### 1-1 텐서를 생성하고 텐서로 변환하는 방법을 이해 및 실습\n",
    "\n",
    "> Random 한 값을 가지는 텐서를 생성하고, list 나 numpy array 같은 다양한 형태의 배열들을 PyTorch 를 이용하여 텐서로 변환하는 과정을 알아봅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKm8y6UjTGgY"
   },
   "source": [
    "#### 📝 설명 : 텐서의 값을 무작위로 생성하는 방법들\n",
    "* rand :  0과 1 사이의 균일한 분포 (Uniform Distribution) 에서 무작위로 생성된 텐서를 반환\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [rand] https://pytorch.org/docs/stable/generated/torch.rand.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 659,
     "status": "ok",
     "timestamp": 1689144284201,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "jLRXK4HLrxFA",
    "outputId": "b4963a07-439c-435c-c69e-4fd77d755185"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5464, 0.7285, 0.3431],\n",
       "        [0.6388, 0.8267, 0.7240]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0부터 1 사이의 값을 랜덤하게 NxM 텐서로 반환\n",
    "torch.rand(2, 3) # torch.rand(NxM) NxM은 텐서의 크기를 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_z19rLVTR2J"
   },
   "source": [
    "#### 📝 설명 : Tensor 의 값을 무작위로 생성하는 방법들\n",
    "* randn : 평균이 0이고 표준 편차가 1인 정규 분포(가우시안 분포)에서 무작위로 생성된 텐서를 반환\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [randn] https://pytorch.org/docs/stable/generated/torch.randn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1689144320910,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "8ipRPNgTIKB2",
    "outputId": "54d9f7b7-57eb-461d-a5b4-820e6b5c7f3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4551, -0.6167, -1.8847],\n",
       "        [ 1.5272,  0.6315,  1.1790]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가우시안 분포에서 렌덤하게 값을 추출 후, NxM 텐서로 반환\n",
    "torch.randn(2, 3) # torch.randn(NxM) NxM은 텐서의 크기를 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLeDGlVPTdgH"
   },
   "source": [
    "#### 📝 설명 : 텐서의 값을 무작위로 생성하는 방법들\n",
    "\n",
    "* randint : 주어진 범위 내에서 정수값을 무작위로 선택하여 텐서를 생성 (단, 최솟값을 포함하고, 최댓값은 포함하지 않음)\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "\n",
    "* [randint] https://pytorch.org/docs/stable/generated/torch.randint.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1689144380968,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "olZplQevJFf1",
    "outputId": "56480bfc-290d-41bb-bdba-e5931bf2742b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 9, 3, 3, 4],\n",
       "        [3, 4, 1, 2, 4],\n",
       "        [9, 7, 7, 7, 9],\n",
       "        [5, 6, 2, 8, 2],\n",
       "        [7, 1, 1, 6, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 범위 내의 정수를 N x M 텐서로 반환\n",
    "torch.randint(1, 10, (5, 5)) # 생성 가능한 최솟값 : 1, 최댓값 : 9, (5x5) Tensor 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwp8WBDQYuSW"
   },
   "source": [
    "#### 📝 설명 : 텐서의 값을 지정해서 생성하는 방법들\n",
    "* zeros : 모든 요소가 0인 텐서 반환\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [zeros] https://pytorch.org/docs/stable/generated/torch.zeros.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1689144401178,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "3YyNbuqYMQ1m",
    "outputId": "5a1d1731-5faf-4ec4-90f1-c3714eece5cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3, 3) # torch.zeros(*size) 여기서 size 는 \",\"로 구분하며 차원을 여러개로 늘릴 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4exBo4-WY3Ed"
   },
   "source": [
    "#### 📝 설명 : 텐서의 값을 지정해서 생성하는 방법들\n",
    "* ones : 모든 요소가 1인 텐서 반환\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [ones] https://pytorch.org/docs/stable/generated/torch.ones.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1689144424393,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "4Q3JI_JYOj7z",
    "outputId": "ef625ddf-c19b-4f04-d4f9-b34ad3186081"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 2, 2) # torch.ones(*size) 여기서 size 는 \",\"로 구분하며 채널을 여러개로 늘릴 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyaKU86RZANI"
   },
   "source": [
    "#### 📝 설명 : 텐서의 값을 지정해서 생성하는 방법들\n",
    "* full: 모든 요소가 지정된 값인 텐서 반환\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [full] https://pytorch.org/docs/stable/generated/torch.full.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1689144460687,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "uEORgXbCOvdc",
    "outputId": "c8755b6f-d4c3-4de2-d89f-a6cc4d7e9578"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5],\n",
       "        [5, 5, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 3), 5) # torch.full((size),value) => 괄호로 텐서의 크기 (2,3) 를 입력하고, 지정한 값 value (5) 로 모든 요소가 설정됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0s5KqAWKZJsz"
   },
   "source": [
    "#### 📝 설명 : 텐서의 값을 지정해서 생성하는 방법들\n",
    "* eye : 단위 행렬 반환 (※ 단위 행렬이란? 대각선 요소가 1이고, 나머지 요소가 0인 행렬)\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [eye] https://pytorch.org/docs/stable/generated/torch.eye.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689144545526,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "a_mZvxquPF-D",
    "outputId": "5ecd9fef-5dee-480e-b806-a1856ce74188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3) # torch.eye(n) (nxn) 크기를 가지는 단위 행렬 반환, 단위행렬 특성 상 정사각행렬 (square matrix)만 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGC1NvljZdCW"
   },
   "source": [
    "#### 📝 설명 : 다양한 데이터를 텐서 형식으로 변환하기\n",
    "* tensor : 주어진 데이터를 텐서로 변환. 데이터는 list, tuple, numpy array 등의 형태일 수 있음.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [tensor] https://pytorch.org/docs/stable/generated/torch.tensor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1689144637195,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "Ytl5SeTnVdV9",
    "outputId": "93dddc3f-483f-4ff3-80b6-3ed31cd1e26d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n",
      "\n",
      "\n",
      "tensor([1, 2, 3])\n",
      "\n",
      "\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# list, tuple, numpy array를 텐서로 바꾸기\n",
    "ls = [[1, 2, 3, 4, 5],[6, 7, 8, 9, 10]] # sample list 생성\n",
    "tup = tuple([1, 2, 3]) # sample tuple 생성\n",
    "arr = np.array([[[1, 2, 3],[4, 5, 6]],[[7, 8, 9],[10, 11, 12]]]) # sample numpy array 생성\n",
    "\n",
    "print(torch.tensor(ls))\n",
    "print('\\n')\n",
    "print(torch.tensor(tup))\n",
    "print('\\n')\n",
    "print(torch.tensor(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7p276DmZlZ4"
   },
   "source": [
    "#### 📝 설명 : 다양한 형태를 텐서 형식으로 변환하기\n",
    "* from_numpy : numpy array 를 텐서로 변환\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [from_numpy] https://pytorch.org/docs/stable/generated/torch.from_numpy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1689144667382,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "0MUst73hXFWO",
    "outputId": "361278e5-c1b2-4b2e-948a-4959379da438"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(arr) # array 를 tensor로 바꾸기 (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWVtMesIZuo2"
   },
   "source": [
    "#### 📝 설명: 다양한 형식의 텐서 변환\n",
    "* as_tensor: 변환 전 데이터와의 메모리 공유(memory sharing)를 사용하므로, 변환 전 데이터 변경 시 변환되어 있는 텐서에도 반영됨\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [as_tensor] https://pytorch.org/docs/stable/generated/torch.as_tensor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1689144732202,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "FUGBYEToXYG6",
    "outputId": "e078826b-c21e-43e8-bae1-6824863e2dd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.tensor\n",
      "tensor([1, 2, 3, 4, 5])\n",
      "----------------------------------------------------------------------\n",
      "torch.as_tensor\n",
      "tensor([10,  2,  3,  4,  5])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor 와 torch.as_tensor 의 차이점 알아보기\n",
    "print(\"torch.tensor\")\n",
    "data1 = np.array([1, 2, 3, 4, 5]) # 샘플 데이터 리스트 생성\n",
    "tensor1 = torch.tensor(data1) # memory 공유 X\n",
    "data1[0] = 10  # 원본 데이터 변경\n",
    "print(tensor1)  # 원본 데이터의 값 변경에 영향을 받지 않음\n",
    "\n",
    "print('-------'*10)\n",
    "\n",
    "print(\"torch.as_tensor\")\n",
    "data2 = np.array([1, 2, 3, 4, 5])\n",
    "tensor2 = torch.as_tensor(data2) # memory 공유 O\n",
    "data2[0] = 10  # 원본 데이터 변경\n",
    "print(tensor2)  # 원본 데이터의 값 변경에 영향을 받음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL2tc5hfZ3Kt"
   },
   "source": [
    "#### 📝 설명 : 다양한 형식의 텐서 변환\n",
    "* Tensor : float32 type으로 텐서 변환\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [Tensor] https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1689144895244,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "nSDhxbyfZhP1",
    "outputId": "5121af3a-5abe-4d33-b7f7-353d109fe27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.tensor\n",
      "Output: tensor([1, 2, 3, 4, 5])\n",
      "Type torch.int64\n",
      "---------------------\n",
      "torch.Tensor\n",
      "Output: tensor([1., 2., 3., 4., 5.])\n",
      "Type torch.float32\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "tensor1 = torch.tensor(data) # list 에서 Tensor 변환\n",
    "print(\"torch.tensor\")\n",
    "print(\"Output:\", tensor1)\n",
    "print(\"Type\", tensor1.dtype) # dtype : Tensor 안의 원소들의 자료형, torch.tensor 는 원본의 데이터 타입을 그대로 따라감\n",
    "\n",
    "print('-------'*3)\n",
    "\n",
    "tensor2 = torch.Tensor(data) # list 에서 Tensor 변환\n",
    "print(\"torch.Tensor\")\n",
    "print(\"Output:\", tensor2)\n",
    "print(\"Type\", tensor2.dtype) # torch.tensor 는 float32 타입으로 Tensor 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ss9HhzieBDH"
   },
   "source": [
    "### 1-2 텐서에서의 Indexing 을 이해 및 실습\n",
    "\n",
    "> Indexing 개념과 Indexing 을 통해 값을 변경하는 방법에 대해 이해하고 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYvdKSEBgjkc"
   },
   "source": [
    "#### 📝 설명 : Indexing 이란?\n",
    "Indexing 은 텐서 내의 특정 **요소**를 index를 통해 접근할 수 있는 방법을 의미합니다.\n",
    "* Indexing 기본 : **대괄호(\"[ ]\")**를 통해 이뤄지며, **\":\"** 는 특정 범위의 접근을 의미합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [Tensor indexing] : https://pytorch.org/cppdocs/notes/tensor_indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689144993162,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "EXYM5UoniFg_",
    "outputId": "2fcedbb4-e512-4dfb-ad2b-491bd42eb691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(5)\n",
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서에서 Indexing 하기\n",
    "tmp_1dim = torch.tensor([i for i in range(10)]) # 0부터 9 까지의 값을 가지는 1차원 텐서 생성\n",
    "\n",
    "print(tmp_1dim[0]) # 첫번째 원소 값 추출\n",
    "print(tmp_1dim[5]) # 6번째 원소 값 추출\n",
    "print(tmp_1dim[-1]) # -1 번째 원소 값 (뒤에서 첫번째) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1689145185788,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "VrBG4imc5TsZ",
    "outputId": "8f296a04-7b60-4078-b3fb-f4818ff22ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape :  torch.Size([4, 3, 2])\n",
      "tensor([[[-0.0058,  0.1709],\n",
      "         [-0.1665, -0.8548],\n",
      "         [-0.7878,  1.3761]],\n",
      "\n",
      "        [[-0.2662,  1.5185],\n",
      "         [-0.3046, -0.8529],\n",
      "         [-0.1896,  0.0368]],\n",
      "\n",
      "        [[ 2.4123, -2.3346],\n",
      "         [-0.2053,  0.6138],\n",
      "         [ 0.6562,  0.5488]],\n",
      "\n",
      "        [[-1.2356,  1.6355],\n",
      "         [-1.8458, -0.2584],\n",
      "         [-0.4446, -0.1554]]])\n",
      "--------------------------------------------------------\n",
      "torch.Size([4, 3])\n",
      "tensor([[-0.0058, -0.1665, -0.7878],\n",
      "        [-0.2662, -0.3046, -0.1896],\n",
      "        [ 2.4123, -0.2053,  0.6562],\n",
      "        [-1.2356, -1.8458, -0.4446]])\n",
      "\n",
      "\n",
      "torch.Size([3])\n",
      "tensor([ 0.1709, -0.8548,  1.3761])\n"
     ]
    }
   ],
   "source": [
    "# 3차원 텐서에서 Indexing 하기\n",
    "tmp_3dim = torch.randn(4, 3, 2) # 4채널, 3행, 2열\n",
    "print(\"Shape : \", tmp_3dim.shape)\n",
    "print(tmp_3dim)\n",
    "\n",
    "print('-------'*8)\n",
    "\n",
    "print(tmp_3dim[:,:,0].shape)\n",
    "print(tmp_3dim[:,:,0]) # 전체 채널과 전체 행에서 0번째 열만 추출\n",
    "\n",
    "print('\\n') # 줄 띄움\n",
    "\n",
    "print(tmp_3dim[0,:,1].shape)\n",
    "print(tmp_3dim[0,:,1])  # 0번째 채널의 전체 행에서 1번째 열만 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tw7YD1ElhaL0"
   },
   "source": [
    "#### 📝 설명 : Indexing 이란?\n",
    "* index_select : 선택한 차원에서 인덱스에 해당하는 요소만 추출하는 함수\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [index_select] : https://pytorch.org/docs/stable/generated/torch.index_select.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1689145264268,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "m2Rmp3wpAXRk",
    "outputId": "2cf5e5a0-7452-40e9-ab3b-8846bd1af114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  2],\n",
       "        [10, 12]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index_select\n",
    "tmp_2dim = torch.tensor([[i for i in range(10)],[i for i in range(10, 20)]])\n",
    "print(tmp_2dim)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "my_index = torch.tensor([0, 2]) # 선택하고자 하는 index 는 텐서 형태이어야 함.\n",
    "torch.index_select(tmp_2dim, dim=1, index=my_index) # 열을 기준으로 0열과 2열을 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oydX5fshNjz"
   },
   "source": [
    "#### 📝 설명 : Indexing 이란?\n",
    "* Masking 을 이용한 Indexing : 조건에 따른 텐서의 요소를 사용하기 위한 방법으로 조건에 맞는 요소들만 반환하는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1689145330029,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "uBhzYlSt8M5J",
    "outputId": "7216f7f6-a400-4b95-d83c-77dc2e94547e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask 를 이용한 텐서 Indexing (조건에 맞는 값만 추출)\n",
    "mask = tmp_2dim >= 5 # 5보다 큰 텐서만 추출\n",
    "tmp_2dim[mask] # 1차원 Tensor 로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPHHskochuNC"
   },
   "source": [
    "#### 📝 설명 : Indexing 이란?\n",
    "* masked_select : 주어진 mask에 해당하는 요소들을 추출하여 1차원으로 펼친 새로운 텐서를 반환하는 함수\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [masked_select] : https://pytorch.org/docs/stable/generated/torch.masked_select.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1689145356311,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "SOHBPOy9BubK",
    "outputId": "38359d00-6b4a-4511-f34b-06cf68be4df2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.masked_select(tmp_2dim, mask = mask) # tmp_2dim[tmp_2dim >= 5] 와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA_afHzNhx7D"
   },
   "source": [
    "#### 📝 설명 : Indexing 이란?\n",
    "* take : 주어진 인덱스를 사용하여 텐서에서 요소를 선택하는 함수. 인덱스 번호는 텐서를 1차원으로 늘려졌을 때 기준으로 접근해야합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [take] : https://pytorch.org/docs/stable/generated/torch.take.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 769,
     "status": "ok",
     "timestamp": 1689145434775,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "8v-7FODdCRb2",
    "outputId": "26d51da6-00cf-4041-d4e8-a8924f488499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 15])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_2dim = torch.tensor([[i for i in range(10)], [i for i in range(10, 20)]])\n",
    "print(tmp_2dim)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "my_index = torch.tensor([0, 15])\n",
    "torch.take(tmp_2dim, index = my_index) # Tensor가 1차원으로 늘려졌을 때 기준으로 index 번호로 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnXjy1yhh1AK"
   },
   "source": [
    "#### 📝 설명 : Indexing 이란?\n",
    "* gather : 주어진 차원에서 인덱스에 해당하는 요소들을 선택하여 새로운 텐서를 반환\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [gather] : https://pytorch.org/docs/stable/generated/torch.gather.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1689145674626,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "f9DRQiZ7UzvF",
    "outputId": "46fafa51-e723-4c71-b901-602889bc5f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
      "\n",
      "\n",
      "tensor([[0, 1],\n",
      "        [9, 8]])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [19, 18]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_2dim = torch.tensor([[i for i in range(10)],[i for i in range(10,20)]])\n",
    "print(tmp_2dim)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "recon_index =  torch.tensor([[0 ,1],[9, 8]]) # 0번째 값, 1번 째 값을 0번째 행으로 설정하고, 9번째 값, 8번째 값을 1번째 행으로 설정한다.\n",
    "dim = 1 # 열 기준\n",
    "print(recon_index)\n",
    "print('\\n')\n",
    "\n",
    "torch.gather(tmp_2dim, dim = 1, index = recon_index) # dim =1 이므로 열 기준, 0행 0열, 0행 1열 선택, 1행 9열, 1행 8열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRtanbVaT39w"
   },
   "source": [
    "## 2. 텐서의 모양 바꾸기\n",
    "\n",
    "```\n",
    "💡 목차 개요 : 텐서의 모양을 바꾸는 방법에 대해 실습을 통해 이해하고, 비슷한 역할을 하는 함수들의 각 차이점에 대해서 알아봅니다.\n",
    "```\n",
    "\n",
    "- 2-1. 텐서의 shape 을 바꾸는 여러가지 함수 이해 및 실습\n",
    "- 2-2. 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "- 2-3. 역할이 비슷한 함수들의 차이 이해 및 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypym_EzHcHsm"
   },
   "source": [
    "### 2-1 텐서의 shape을 바꾸는 여러가지 함수 이해 및 실습\n",
    "> 텐서의 모양을 자유자재로 바꾸는 방법에 대해 알아보고 실습합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL5epVK-i0Dm"
   },
   "source": [
    "#### 📝 설명 : 텐서의 shape 변경\n",
    "텐서에 대한 모양을 변경하기 위해 명심해야 할 점은 텐서의 크기 (요소의 개수)는 유지되어야 한다는 점입니다.\n",
    "* size : 텐서의 모양을 확인합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [size] : https://pytorch.org/docs/stable/generated/torch.Tensor.size.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1689145794563,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "ipwFOrsidAE7",
    "outputId": "b99d7dc0-4160-4c47-e5e1-e1c607a5bce4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 5) # random 한 값을 가진 (1,3,5) 텐서 생성\n",
    "a.size() # 차원 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1689145796827,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "8Bx4agDadDYD",
    "outputId": "d45dff02-a683-4e29-dc35-f3bdd169984b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape # a.size() 와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUibyB1ki9fy"
   },
   "source": [
    "#### 📝 설명 : 텐서의 shape 변경\n",
    "* reshape : 텐서의 모양을 변경합니다. 메모리를 공유하지 않습니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [reshape] : https://pytorch.org/docs/stable/generated/torch.reshape.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1689145937577,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "KM-sMX3Ug0WP",
    "outputId": "64e5577e-593c-4ee3-8a80-859be8cca5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8712,  0.1725,  0.0584,  0.5019, -1.2336],\n",
      "         [-1.2567,  0.1898, -1.6370, -0.4722,  0.3590],\n",
      "         [-2.9057,  0.0933,  0.9549,  0.5079,  0.9008]],\n",
      "\n",
      "        [[-0.3023, -1.6888,  0.2801,  0.1265, -0.3326],\n",
      "         [-0.3450,  1.7740, -0.9367, -0.0679, -0.5905],\n",
      "         [ 0.7125,  0.1870, -0.0377, -0.3762,  1.7022]]])\n",
      "Shape :  torch.Size([2, 3, 5])\n",
      "\n",
      "\n",
      "tensor([[ 1.8712,  0.1725,  0.0584,  0.5019, -1.2336, -1.2567],\n",
      "        [ 0.1898, -1.6370, -0.4722,  0.3590, -2.9057,  0.0933],\n",
      "        [ 0.9549,  0.5079,  0.9008, -0.3023, -1.6888,  0.2801],\n",
      "        [ 0.1265, -0.3326, -0.3450,  1.7740, -0.9367, -0.0679],\n",
      "        [-0.5905,  0.7125,  0.1870, -0.0377, -0.3762,  1.7022]])\n",
      "Shape :  torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "# 모양 변경\n",
    "a = torch.randn(2, 3, 5) # (2,3,5) 크기를 가지는 텐서 생성\n",
    "print(a)\n",
    "print(\"Shape : \", a.size()) # 텐서 모양 반환\n",
    "print('\\n')\n",
    "\n",
    "reshape_a = a.reshape(5, 6) # 3차원 텐서를 2차원 텐서로 크기 변경 (2,3,5) -> (5,6)\n",
    "print(reshape_a)\n",
    "print(\"Shape : \", reshape_a.size()) # 변경한 텐서 모양 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1689145995242,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "M36tGjarh9I7",
    "outputId": "ca879151-d76c-448e-b727-d1be9105ded7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "# -1 로 모양 자동 설정\n",
    "reshape_auto_a = a.reshape(3, -1) # (2,3,5) 크기를 가지는 Tensor를 (3,n)의 모양으로 변경, \"-1\" 로 크기 자동 계산\n",
    "print(reshape_auto_a.size()) # 2x3x5 = 3 x n 의 방정식을 푸는 문제로 n 이 자동설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "error",
     "timestamp": 1689146014861,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "VSviAQIbic9h",
    "outputId": "fbcb9323-6555-40f5-90c6-02ef7eacea74"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7a4b40636089>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#  2x3x5 = 3 x n 의 방정식의 해가 정수가 아니면 오류 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[7, -1]' is invalid for input of size 30"
     ]
    }
   ],
   "source": [
    "a.reshape(7, -1) #  2x3x5 = 3 x n 의 방정식의 해가 정수가 아니면 오류 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpQFHxrIjJVe"
   },
   "source": [
    "#### 📝 설명 : 텐서의 shape 변경\n",
    "* view : 텐서의 모양을 변경합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [view] : https://pytorch.org/docs/stable/generated/torch.Tensor.view.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1689146029645,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "rbmti7R6pvxy",
    "outputId": "47314b4b-8417-4424-dae5-41684bfa66a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8712,  0.1725,  0.0584,  0.5019, -1.2336],\n",
      "         [-1.2567,  0.1898, -1.6370, -0.4722,  0.3590],\n",
      "         [-2.9057,  0.0933,  0.9549,  0.5079,  0.9008]],\n",
      "\n",
      "        [[-0.3023, -1.6888,  0.2801,  0.1265, -0.3326],\n",
      "         [-0.3450,  1.7740, -0.9367, -0.0679, -0.5905],\n",
      "         [ 0.7125,  0.1870, -0.0377, -0.3762,  1.7022]]])\n",
      "Shape :  torch.Size([2, 3, 5])\n",
      "\n",
      "\n",
      "tensor([[ 1.8712,  0.1725,  0.0584,  0.5019, -1.2336, -1.2567],\n",
      "        [ 0.1898, -1.6370, -0.4722,  0.3590, -2.9057,  0.0933],\n",
      "        [ 0.9549,  0.5079,  0.9008, -0.3023, -1.6888,  0.2801],\n",
      "        [ 0.1265, -0.3326, -0.3450,  1.7740, -0.9367, -0.0679],\n",
      "        [-0.5905,  0.7125,  0.1870, -0.0377, -0.3762,  1.7022]])\n",
      "Shape :  torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(\"Shape : \", a.size()) # 텐서 모양 반환\n",
    "print('\\n')\n",
    "\n",
    "view_a = a.view(5, 6) # reshape 과 동일하게 (2,3,5) 크기를 (5,6) 크기로 변경\n",
    "print(view_a)\n",
    "print(\"Shape : \", view_a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1689146041894,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "QbPBudrtqnqe",
    "outputId": "46681482-c252-4c19-d567-7fa8351b30b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "view_auto_a = a.view(3, -1) # reshape 과 동일하게 (3,n)의 모양으로 변경. \"-1\" 로 크기 자동 계산\n",
    "print(view_auto_a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpfbpFqyjio4"
   },
   "source": [
    "#### 📝 설명 : 텐서의 shape 변경\n",
    "* transpose : 텐서의 차원을 전치합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [transpose] : https://pytorch.org/docs/stable/generated/torch.transpose.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1689146162154,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "fP86P3hdi2S3",
    "outputId": "4c53301f-6f53-457f-d197-10c2f79f5dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 9, 1, 8],\n",
      "         [7, 9, 1, 9, 3]],\n",
      "\n",
      "        [[7, 8, 6, 3, 3],\n",
      "         [6, 1, 3, 8, 9]],\n",
      "\n",
      "        [[9, 6, 2, 8, 4],\n",
      "         [6, 9, 7, 1, 8]]])\n",
      "Shape :  torch.Size([3, 2, 5])\n",
      "\n",
      "\n",
      "tensor([[[1, 7],\n",
      "         [2, 9],\n",
      "         [9, 1],\n",
      "         [1, 9],\n",
      "         [8, 3]],\n",
      "\n",
      "        [[7, 6],\n",
      "         [8, 1],\n",
      "         [6, 3],\n",
      "         [3, 8],\n",
      "         [3, 9]],\n",
      "\n",
      "        [[9, 6],\n",
      "         [6, 9],\n",
      "         [2, 7],\n",
      "         [8, 1],\n",
      "         [4, 8]]])\n",
      "Shape :  torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randint(1, 10, (3, 2, 5)) # 1 ~ 9의 값을 가지는 (3,2,5) 사이즈의 Tensor 생성\n",
    "print(tensor_a)\n",
    "print(\"Shape : \", tensor_a.size())\n",
    "print('\\n')\n",
    "\n",
    "# (3,2,5) 를 (2,3,5) 의 크기로 변경\n",
    "trans_a = tensor_a.transpose(1, 2) # 행과 열을 서로 전치, 서로 전치할 차원 2개를 지정\n",
    "print(trans_a)\n",
    "print(\"Shape : \", trans_a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_R9R2Spjeon"
   },
   "source": [
    "#### 📝 설명 : 텐서의 shape 변경\n",
    "* permute : 텐서 차원의 순서를 재배열합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [permute] : https://pytorch.org/docs/stable/generated/torch.permute.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689146165226,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "h7Z_fH0oo8Ij",
    "outputId": "3174ec17-cf57-4e34-b1ae-8b53b801dbec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 9, 1, 8],\n",
      "         [7, 9, 1, 9, 3]],\n",
      "\n",
      "        [[7, 8, 6, 3, 3],\n",
      "         [6, 1, 3, 8, 9]],\n",
      "\n",
      "        [[9, 6, 2, 8, 4],\n",
      "         [6, 9, 7, 1, 8]]])\n",
      "Shape :  torch.Size([3, 2, 5])\n",
      "\n",
      "\n",
      "tensor([[[1, 7],\n",
      "         [2, 9],\n",
      "         [9, 1],\n",
      "         [1, 9],\n",
      "         [8, 3]],\n",
      "\n",
      "        [[7, 6],\n",
      "         [8, 1],\n",
      "         [6, 3],\n",
      "         [3, 8],\n",
      "         [3, 9]],\n",
      "\n",
      "        [[9, 6],\n",
      "         [6, 9],\n",
      "         [2, 7],\n",
      "         [8, 1],\n",
      "         [4, 8]]])\n",
      "Shape :  torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_a)\n",
    "print(\"Shape : \", tensor_a.size())\n",
    "print('\\n')\n",
    "\n",
    "permute_a = tensor_a.permute(0, 2, 1) # (3,2,5)의 모양을 (3,5,2)의 모양으로 변경\n",
    "print(permute_a)\n",
    "print(\"Shape : \", permute_a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0zh9ojKkjbQ"
   },
   "source": [
    "### 2-2 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "> 텐서의 차원 변경 방식에 대한 이해와 활용을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOiKDQoAp2XW"
   },
   "source": [
    "#### 📝 설명 : 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "* unsqueeze : 텐서에 특정 차원에 크기가 1인 차원을 추가합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [unsqueeze] : https://pytorch.org/docs/stable/generated/torch.unsqueeze.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1689146234629,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "QpUG5J0Hs3GK",
    "outputId": "3c9ff20a-5f7a-4ff5-d167-b9c072cc2bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n",
      "Shape :  torch.Size([5, 2])\n",
      "\n",
      "\n",
      "tensor([[[0, 1],\n",
      "         [2, 3],\n",
      "         [4, 5],\n",
      "         [6, 7],\n",
      "         [8, 9]]])\n",
      "Shape :  torch.Size([1, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([i for i in range(10)]).reshape(5, 2) # 0부터 9까지의 숫자들을 (5,2) 크기로 변경\n",
    "print(tensor_a)\n",
    "print('Shape : ', tensor_a.size())\n",
    "print('\\n')\n",
    "\n",
    "unsqu_a = tensor_a.unsqueeze(0) # 0번째 차원 하나 추가 (5,2) => (1,5,2)\n",
    "print(unsqu_a)\n",
    "print('Shape : ', unsqu_a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1689146268849,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "dbR8cf0Luwnq",
    "outputId": "c2d4adcf-94b5-488f-d815-89e3892806ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0],\n",
      "         [1]],\n",
      "\n",
      "        [[2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5]],\n",
      "\n",
      "        [[6],\n",
      "         [7]],\n",
      "\n",
      "        [[8],\n",
      "         [9]]])\n",
      "Shape :  torch.Size([5, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "unsqu_a2 = tensor_a.unsqueeze(-1) # 마지막번째에 차원 하나 추가 (5,2) => (5,2,1)\n",
    "print(unsqu_a2)\n",
    "print('Shape : ', unsqu_a2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2uRteAxkxWf"
   },
   "source": [
    "#### 📝 설명 : 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "* squeeze : 텐서에 차원의 크기가 1인 차원을 제거합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [squeeze] : https://pytorch.org/docs/stable/generated/torch.squeeze.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1688752218232,
     "user": {
      "displayName": "JaeHyun Lee",
      "userId": "13762872260843906568"
     },
     "user_tz": -540
    },
    "id": "kPJxopmYBxuh",
    "outputId": "51191f9a-82b3-4ab5-a3a7-a16f94ea4000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1],\n",
      "         [2, 3],\n",
      "         [4, 5],\n",
      "         [6, 7],\n",
      "         [8, 9]]])\n",
      "Shape :  torch.Size([1, 5, 2])\n",
      "\n",
      "\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n",
      "Shape :  torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(unsqu_a)\n",
    "print(\"Shape : \", unsqu_a.size())\n",
    "print('\\n')\n",
    "\n",
    "squ = unsqu_a.squeeze() # 차원이 1인 차원을 제거\n",
    "print(squ)\n",
    "print(\"Shape : \", squ.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1688752218232,
     "user": {
      "displayName": "JaeHyun Lee",
      "userId": "13762872260843906568"
     },
     "user_tz": -540
    },
    "id": "7L5ZDMRuwEXc",
    "outputId": "3e5b739b-66dd-4da8-fc30-66e20a8db6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (original) :  torch.Size([2, 1, 2, 1, 2])\n",
      "\n",
      "\n",
      "Shape (squeeze()) : torch.Size([2, 2, 2])\n",
      "\n",
      "\n",
      "Shape (squeeze(0)) : torch.Size([2, 1, 2, 1, 2])\n",
      "\n",
      "\n",
      "Shape (squeeze(1)) : torch.Size([2, 2, 1, 2])\n",
      "\n",
      "\n",
      "Shape (squeeze(0,1,3)) : torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2) # 모든 원소가 0인 (2,1,2,1,2) 크기를 가지는 텐서\n",
    "print(\"Shape (original) : \", x.size()) # 원래 텐서 크기\n",
    "print('\\n')\n",
    "\n",
    "print(\"Shape (squeeze()) :\", x.squeeze().size()) # 차원이 1인 차원이 여러개일 때, 모든 차원이 1인 차원 제거\n",
    "print('\\n')\n",
    "\n",
    "print(\"Shape (squeeze(0)) :\", x.squeeze(0).size()) # 0번째 차원은 차원의 크기가 1이 아니므로, 변화 없음\n",
    "print('\\n')\n",
    "\n",
    "print(\"Shape (squeeze(1)) :\", x.squeeze(1).size()) # 1번째 차원은 차원의 크기가 1이므로 제거\n",
    "print('\\n')\n",
    "\n",
    "print(\"Shape (squeeze(0,1,3)) :\", x.squeeze((0, 1, 3)).size()) # 여러 차원 제거 가능 (0번째 차원은 차원의 크기가 1이 아니기 때문에 무시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVctYQQrk1YV"
   },
   "source": [
    "#### 📝 설명 : 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "* expand : 텐서의 값을 반복하여 크기를 확장합니다.\n",
    "  * A 텐서가 1차원일 경우 : A 텐서의 크기가 (m,) 이면 m은 고정하고 (x,m)의 크기로만 확장 가능\n",
    "  * A 텐서가 2차원 이상일 경우 : 크기가 1인 차원에 대해서만 적용 가능. A 텐서의 크기가 (1,m) 이면 (x,m) , (m,1) 이면 (m,y) 로만 확장 가능.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [expand] : https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689146489609,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "ZJIJUuZLxanz",
    "outputId": "e9aaab48-cf52-461b-d427-593089d5be8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "Shape :  torch.Size([4])\n",
      "\n",
      "\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 2, 3, 4]])\n",
      "Shape :  torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_1dim = torch.tensor([1, 2, 3, 4])\n",
    "print(tensor_1dim)\n",
    "print(\"Shape : \", tensor_1dim.size())\n",
    "print('\\n')\n",
    "\n",
    "expand_tensor = tensor_1dim.expand(3, 4) # (,4) 를 (3,4) 의 크기로 확장 (값을 반복)\n",
    "print(expand_tensor)\n",
    "print(\"Shape : \", expand_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "error",
     "timestamp": 1689146569339,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "PYRKOkFlPXMY",
    "outputId": "247fc402-d8d8-428f-d120-02398ca06510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [1, 2, 3, 4]])\n",
      "Shape :  torch.Size([2, 4])\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ca0aa7093d8c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mexpand_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_2dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (2,4) 를 (4,8) 의 크기로 확장 (값을 반복)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 에러 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 에러 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [4, 4].  Tensor sizes: [2, 4]"
     ]
    }
   ],
   "source": [
    "tensor_2dim = torch.tensor([[1, 2, 3, 4], [1, 2, 3, 4]]) # (2,4) 크기를 가진 Tensor\n",
    "print(tensor_2dim)\n",
    "print(\"Shape : \", tensor_2dim.size())\n",
    "print('\\n')\n",
    "\n",
    "expand_tensor = tensor_2dim.expand(4,4) # (2,4) 를 (4,8) 의 크기로 확장 (값을 반복)\n",
    "print(expand_tensor) # 에러 발생\n",
    "print(\"Shape : \", expand_tensor.size()) # 에러 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-J0mAZ6k4OF"
   },
   "source": [
    "#### 📝 설명 : 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "* repeat : 텐서를 반복하여 크기를 확장합니다.\n",
    "  * ex) A 텐서가 (m,n) 크기를 가진다하고, A 텐서를 repeat(i,j) 를 하면 결과값으로 (m x i, n x j)의 크기의 텐서가 생성됩니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [repeat] : https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689146615597,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "tJNvShg4ypAS",
    "outputId": "51df633c-9565-4d7f-b3bd-fbcabe0aabe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "Shape :  torch.Size([4])\n",
      "\n",
      "\n",
      "tensor([[1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n",
      "        [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n",
      "        [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]])\n",
      "Shape :  torch.Size([3, 16])\n"
     ]
    }
   ],
   "source": [
    "tensor_1dim = torch.tensor([1, 2, 3, 4])\n",
    "print(tensor_1dim)\n",
    "print(\"Shape : \", tensor_1dim.size())\n",
    "print('\\n')\n",
    "\n",
    "repeat_tensor = tensor_1dim.repeat(3, 4) # tensor_1dim 자체를 행으로 3번 반복, 열로 4번 반복\n",
    "print(repeat_tensor)\n",
    "print(\"Shape : \", repeat_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83i37MGBk6lC"
   },
   "source": [
    "#### 📝 설명 : 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "* flatten : 다차원 텐서를 1차원 텐서로 변경합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [flatten] : https://pytorch.org/docs/stable/generated/torch.flatten.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1689146657316,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "nsZ6l_5kzUvv",
    "outputId": "6fc16724-e88a-41e9-bae1-93a96bee058f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5],\n",
      "         [ 6,  7],\n",
      "         [ 8,  9]],\n",
      "\n",
      "        [[10, 11],\n",
      "         [12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17],\n",
      "         [18, 19]]])\n",
      "Shape :  torch.Size([2, 5, 2])\n",
      "\n",
      "\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "Shape :  torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([i for i in range(20)]).reshape(2, 5, 2) # 0부터 19까지의 숫자를 4행 5열 Tensor로 구성\n",
    "print(t)\n",
    "print(\"Shape : \", t.size())\n",
    "print('\\n')\n",
    "\n",
    "flat_tensor = t.flatten() # (2, 5, 2) 의 Tensor를 (20,)로 모양 변경, 1차원으로 변경\n",
    "print(flat_tensor)\n",
    "print(\"Shape : \", flat_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1688752313543,
     "user": {
      "displayName": "JaeHyun Lee",
      "userId": "13762872260843906568"
     },
     "user_tz": -540
    },
    "id": "JraIVhtI0cCN",
    "outputId": "bd5f3e43-cd55-4c82-8985-7448106e4ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "flat_tensor2 = t.flatten(start_dim=1) # flatten을 시작할 차원을 지정할 수 있음. 지정한 차원 이후의 모든 차원을 하나의 차원으로 평면화, 기본값 = 0 (1차원)\n",
    "print(flat_tensor2)\n",
    "print(flat_tensor2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Vnzd67tk9qn"
   },
   "source": [
    "#### 📝 설명 : 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습\n",
    "* ravel : 다차원 텐서를 1차원 텐서로 변경합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [ravel] : https://pytorch.org/docs/stable/generated/torch.ravel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1689146746775,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "9vQ8tr-_z-eY",
    "outputId": "8593fe1a-2f0d-4871-9a93-7fd918aafc90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5],\n",
      "         [ 6,  7],\n",
      "         [ 8,  9]],\n",
      "\n",
      "        [[10, 11],\n",
      "         [12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17],\n",
      "         [18, 19]]])\n",
      "Shape :  torch.Size([2, 5, 2])\n",
      "\n",
      "\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "Shape :  torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([i for i in range(20)]).reshape(2, 5, 2) # 0부터 19까지의 숫자를 (2, 5, 2) 크기 Tensor로 구성\n",
    "print(t)\n",
    "print(\"Shape : \", t.size())\n",
    "print('\\n')\n",
    "\n",
    "ravel_tensor = t.ravel() # flatten 과 동일하게 (2,5,2) 의 텐서를 (20,)로 모양 변경, 1차원으로 변경\n",
    "print(ravel_tensor)\n",
    "print(\"Shape : \", ravel_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "error",
     "timestamp": 1689146755925,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "bkwkNvwW1r4q",
    "outputId": "fc6a7cfd-6300-4b15-af35-394662229d8b"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4ad2f534a783>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 에러 발생, ravel 은 flatten 과 달리 어떠한 축을 기준으로 평탄화 하는 작업이 없음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: _TensorBase.ravel() takes no arguments (1 given)"
     ]
    }
   ],
   "source": [
    "t.ravel(1) # 에러 발생, ravel 은 flatten 과 달리 어떠한 축을 기준으로 평탄화 하는 작업이 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8ALfH-nkR7L"
   },
   "source": [
    "### 2-3 역할이 비슷한 함수들의 차이 이해 및 실습\n",
    "> 역할이 비슷한 함수들의 공통점과 차이점을 이해하고 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7k7Av2cUiL7"
   },
   "source": [
    "#### 📝 설명 : 역할이 비슷한 함수들의 차이 이해 및 실습\n",
    "* 모양 변경 : view vs. reshape vs. unsqueeze\n",
    "  * ※ contiguous 란?\n",
    "    * 텐서의 메모리 상에 연속적인 데이터 배치를 갖는 것\n",
    "    * 텐서를 처음 생성 후 정의하면 기본적으로 contiguous 하지만, 이에 대해 차원의 순서를 변경하는 과정을 거치면 contiguous 하지 않습니다.\n",
    "    * 텐서의 contiguous 함을 확인하기 위해선 is_contiguous() 를 사용합니다.\n",
    "  * view 는 contiguous 하지 않은 텐서에 대해서 동작하지 않습니다.\n",
    "  * reshape 는 contiguous 하지 않은 텐서를 contiguous 하게 만들어주고, 크기를 변경합니다.\n",
    "  * unsqueeze 는 차원의 크기가 1인 차원을 추가하지만, 차원의 크기가 1이 아니면 차원의 모양을 변경할 수 없습니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [what is contiguous?] : https://titania7777.tistory.com/3\n",
    "* [view vs reshape] :  https://inmoonlight.github.io/2021/03/03/PyTorch-view-transpose-reshape/\n",
    "* [view, reshape, transpose, permute 비교] : https://sanghyu.tistory.com/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "error",
     "timestamp": 1689146930024,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "kOLmpPvq7dW_",
    "outputId": "5ccebce3-9777-44f5-b294-0b8b2df5436f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-58c86dac39d7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtmp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# contiguous 를 False 로 만들기 위한 작업\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# contiguous 한지 검사\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# view는 contiguous 하지 않은 텐서에 대해선 동작이 되지 않음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# view vs reshape\n",
    "tmp = torch.tensor([[[0, 1], [2, 3], [4, 5]], \\\n",
    "                 [[6, 7], [8, 9], [10, 11]], \\\n",
    "                 [[12, 13], [14, 15], [16, 17]], \\\n",
    "                 [[18, 19], [20, 21], [22, 23]]])\n",
    "tmp_t = tmp.transpose(0,1) # contiguous 를 False 로 만들기 위한 작업\n",
    "print(tmp_t.is_contiguous()) # contiguous 한지 검사\n",
    "print(tmp_t.view(-1)) # view는 contiguous 하지 않은 텐서에 대해선 동작이 되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1689146964909,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "pOgMDt6Q_NYO",
    "outputId": "38817f49-5239-4b22-c724-e08f12f5a0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  6,  7, 12, 13, 18, 19,  2,  3,  8,  9, 14, 15, 20, 21,  4,  5,\n",
      "        10, 11, 16, 17, 22, 23])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reshape_tmp = tmp_t.reshape(-1) # reshape은 contiguous 하지 않아도 동작이 됨\n",
    "print(reshape_tmp)\n",
    "print(reshape_tmp.is_contiguous()) # contiguous 하지 않았던 Tensor를 contiguous 하게 변경해 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1689146994151,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "78nn8gTvtZW_",
    "outputId": "3e3ac3d4-459c-4a8b-9f08-e31509c0b588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View output size :  torch.Size([2, 3, 1])\n",
      "Reshape output size :  torch.Size([2, 3, 1])\n",
      "Unsqueeze output size :  torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# (view , reshape) vs unsqueeze\n",
    "tensor_a = torch.randn(2, 3)\n",
    "# (2, 3) 의 텐서를 (2, 3, 1)의 크기로 변경\n",
    "view_tensor = tensor_a.view(2, 3, 1) # view 를 이용하여 (2,3,1) 의 크기로 변경\n",
    "reshape_tensor = tensor_a.reshape(2, 3, 1) # reshape 를 이용하여 (2,3,1) 의 크기로 변경\n",
    "unsqueeze_tensor = tensor_a.unsqueeze(-1) # unsqueeze 를 이용하여 (2,3,1) 의 크기로 변경\n",
    "\n",
    "print(\"View output size : \",view_tensor.size())\n",
    "print(\"Reshape output size : \",reshape_tensor.size())\n",
    "print(\"Unsqueeze output size : \",unsqueeze_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lS3Y-kslMWb"
   },
   "source": [
    "#### 📝 설명 : 역할이 비슷한 함수들의 차이 이해 및 실습\n",
    "* 차원 변경 : transpose vs. permute\n",
    "  * transpose : 두 차원에 대해서만 변경이 가능\n",
    "    * 인자가 총 2개여야함.\n",
    "  * permute : 모든 차원에 대해서 변경이 가능\n",
    "    * 인자가 차원의 개수와 동일해야 함.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [view, reshape, transpose, permute 비교] : https://sanghyu.tistory.com/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1689147084017,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "SjIPZQ753fHA",
    "outputId": "dbd4aff4-f5da-4ed4-84f1-b3e7e3972bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose tensor shape :  torch.Size([2, 2, 3])\n",
      "Permute tensor shape :  torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor_a = torch.randn(2, 3, 2)\n",
    "transpose_tensor = tensor_a.transpose(2, 1) # 행과 열을 전치\n",
    "permute_tensor = tensor_a.permute(0, 2, 1) # 행과 열을 바꿈.\n",
    "\n",
    "print(\"Transpose tensor shape : \", transpose_tensor.size())\n",
    "print(\"Permute tensor shape : \", permute_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_wgTjTal4KZ"
   },
   "source": [
    "#### 📝 설명 : 역할이 비슷한 함수들의 차이 이해 및 실습\n",
    "* 반복을 통한 텐서 크기 확장 : expand vs. repeat\n",
    "  * expand\n",
    "    * 원본 텐서와 메모리를 공유한다.\n",
    "  * repeat\n",
    "    * 원본 텐서와 메모리를 공유하지 않는다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [expand vs repeat] : https://seducinghyeok.tistory.com/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1689147301242,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "-p9i8pdE-2BF",
    "outputId": "db801803-57a4-4e92-966a-4f23a1aa84fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor Size\n",
      "torch.Size([1, 1, 3])\n",
      "tensor([[[0.9457, 0.3915, 0.5611]]])\n",
      "\n",
      "\n",
      "Shape of expanded tensor: torch.Size([4, 1, 3])\n",
      "\n",
      "\n",
      "Shape of repeated tensor: torch.Size([4, 1, 3])\n",
      "\n",
      "\n",
      "Expanded Tensor\n",
      "tensor([[[0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.]]])\n",
      "\n",
      "\n",
      "Repeated Tensor\n",
      "tensor([[[0.9457, 0.3915, 0.5611]],\n",
      "\n",
      "        [[0.9457, 0.3915, 0.5611]],\n",
      "\n",
      "        [[0.9457, 0.3915, 0.5611]],\n",
      "\n",
      "        [[0.9457, 0.3915, 0.5611]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 원본 텐서 생성\n",
    "tensor_a = torch.rand(1, 1, 3)\n",
    "print('Original Tensor Size')\n",
    "print(tensor_a.size())\n",
    "print(tensor_a)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# expand 사용하여 (1,1,3) => (4, 1, 3)\n",
    "expand_tensor = tensor_a.expand(4, 1, -1)\n",
    "print(\"Shape of expanded tensor:\", expand_tensor.size())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# repeat 사용하여 (1,1,3) => (4, 1, 3)\n",
    "repeat_tensor = tensor_a.repeat(4, 1, 1)\n",
    "print(\"Shape of repeated tensor:\", repeat_tensor.size())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# 평면화된 뷰 수정 후 원본 텐서 확인\n",
    "tensor_a[:] = 0\n",
    "\n",
    "print(\"Expanded Tensor\")\n",
    "print(expand_tensor) # 값 변경이 됨\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Repeated Tensor\")\n",
    "print(repeat_tensor) # 깂 변경 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yfcl7ZWnCBo"
   },
   "source": [
    "## 3. 텐서 합치기 나누기\n",
    "\n",
    "```\n",
    "💡 목차 개요 : 하나의 텐서를 여러가지로 나누는 방법과 여러 텐서를 하나의 텐서로 합치는 방법에 대해 이해하고 실습한다.\n",
    "```\n",
    "\n",
    "- 3-1. 여러 텐서를 합치는 방법에 대한 이해 및 실습\n",
    "- 3-2. 하나의 텐서를 여러 텐서로 나누는 방법에 대한 이해 및 실습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIZlhJQInf9e"
   },
   "source": [
    "### 3-1 여러 텐서를 합치는 방법에 대한 이해 및 실습\n",
    "\n",
    "> 여러 텐서를 하나의 텐서로 합쳐서 새로운 텐서를 생성하는 과정을 알아봅니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6GaawUfqZvA"
   },
   "source": [
    "#### 📝 설명 : 여러 텐서 합치기\n",
    "* cat : 주어진 차원을 따라 텐서들을 연결합니다. (주어진 차원 외의 다른 차원의 크기가 같아야합니다.)\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [cat] : https://pytorch.org/docs/stable/generated/torch.cat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1689147386472,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "umLxwbZdqxoR",
    "outputId": "962f07ac-cbdc-4115-b4c4-384bbf724ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A shape :  torch.Size([2, 3])\n",
      "tensor([[4, 2, 8],\n",
      "        [6, 4, 3]])\n",
      "\n",
      "\n",
      "Tensor B shape :  torch.Size([5, 3])\n",
      "tensor([[0.2035, 0.2748, 0.7061],\n",
      "        [0.7608, 0.9373, 0.8164],\n",
      "        [0.1454, 0.4525, 0.1021],\n",
      "        [0.5300, 0.7971, 0.3694],\n",
      "        [0.2201, 0.1876, 0.4208]])\n",
      "\n",
      "\n",
      "Concat Tensor A and B (by row) Shape :  torch.Size([7, 3])\n",
      "tensor([[4.0000, 2.0000, 8.0000],\n",
      "        [6.0000, 4.0000, 3.0000],\n",
      "        [0.2035, 0.2748, 0.7061],\n",
      "        [0.7608, 0.9373, 0.8164],\n",
      "        [0.1454, 0.4525, 0.1021],\n",
      "        [0.5300, 0.7971, 0.3694],\n",
      "        [0.2201, 0.1876, 0.4208]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randint(1, 10, (2, 3)) # 1부터 9까지의 무작위 정수가 있는 (2,3) Tensor\n",
    "tensor_b = torch.rand(5, 3) # 0부터 1까지의 균등분포를 따르는 (5,3) Tensor\n",
    "\n",
    "print(\"Tensor A shape : \", tensor_a.size())\n",
    "print(tensor_a)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Tensor B shape : \", tensor_b.size())\n",
    "print(tensor_b)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "a_cat_b_row = torch.cat((tensor_a, tensor_b), dim=0) # dim = 0 (행), Tensor A 와 Tensor B 를 행 기준으로 합친다.\n",
    "print(\"Concat Tensor A and B (by row) Shape : \", a_cat_b_row.shape) # (Tensor A 행 개수 + Tensor B 행 개수, Tensor A/B 열 개수)\n",
    "print(a_cat_b_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0F2JMPus3Xy"
   },
   "source": [
    "#### 📝 설명 : 여러 텐서 합치기\n",
    "* stack : 주어진 차원을 새로운 차원으로 추가하여 텐서들을 쌓습니다.\n",
    "  * 합쳐질 텐서들의 크기는 모두 같아야합니다.\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [stack] : https://pytorch.org/docs/stable/generated/torch.stack.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1689147476932,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "l8S9Dbrvs3Gx",
    "outputId": "e8afae62-6153-42cc-babc-e1e883291184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A shape :  torch.Size([3, 2])\n",
      "tensor([[8, 7],\n",
      "        [1, 1],\n",
      "        [9, 1]])\n",
      "\n",
      "\n",
      "Tensor B shape :  torch.Size([3, 2])\n",
      "tensor([[0.6636, 0.6241],\n",
      "        [0.6152, 0.6239],\n",
      "        [0.6132, 0.1290]])\n",
      "\n",
      "\n",
      "Stack A and B (by row):  torch.Size([2, 3, 2])\n",
      "tensor([[[8.0000, 7.0000],\n",
      "         [1.0000, 1.0000],\n",
      "         [9.0000, 1.0000]],\n",
      "\n",
      "        [[0.6636, 0.6241],\n",
      "         [0.6152, 0.6239],\n",
      "         [0.6132, 0.1290]]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randint(1, 10, (3, 2))  # 1부터 9까지의 무작위 정수가 있는 (3,2) Tensor\n",
    "tensor_b = torch.rand(3, 2)  # 0부터 1까지의 균등분포를 따르는 (3,2) Tensor\n",
    "\n",
    "print(\"Tensor A shape : \", tensor_a.size())\n",
    "print(tensor_a)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Tensor B shape : \", tensor_b.size())\n",
    "print(tensor_b)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "stack_tensor_row = torch.stack([tensor_a, tensor_b], dim=0)  # dim = 0, 행을 기준으로 Tensor A 에 Tensor B 를 쌓기\n",
    "print(\"Stack A and B (by row): \", stack_tensor_row.size()) # (쌓은 Tensor 개수, Tensor A/B 행 개수, Tensor A/B 열 개수)\n",
    "print(stack_tensor_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jfYxwMLvPJ9"
   },
   "source": [
    "### 3-2. 하나의 텐서를 여러 텐서로 나누는 방법에 대한 이해 및 실습\n",
    "\n",
    "> 하나의 텐서를 다양한 방법을 통해 여러 텐서로 나누는 과정을 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYsK5iOawR6I"
   },
   "source": [
    "#### 📝 설명 : 텐서 나누기\n",
    "* chunk : 나누고자 하는 **텐서의 개수**를 지정하여 원래의 텐서를 개수에 맞게 분리합니다.\n",
    "  * chunks 인자\n",
    "    * 몇 **개**의 텐서로 나눌 것인지\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [chunk] : https://pytorch.org/docs/stable/generated/torch.chunk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1689147563316,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "g7PRrBiFwRB1",
    "outputId": "39526954-5ab0-49b7-8dd1-9315cef1747a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :  tensor([[3, 3, 5, 5],\n",
      "        [3, 8, 7, 7],\n",
      "        [6, 5, 4, 4],\n",
      "        [8, 4, 2, 2],\n",
      "        [9, 6, 3, 1],\n",
      "        [5, 6, 7, 3]])\n",
      "\n",
      "\n",
      "3 개의 Tensor로 분리\n",
      "\n",
      "\n",
      "0 번째 Tensor \n",
      "tensor([[3, 3, 5, 5],\n",
      "        [3, 8, 7, 7]])\n",
      "0 번째 Tensor 크기 torch.Size([2, 4])\n",
      "------------------------------\n",
      "1 번째 Tensor \n",
      "tensor([[6, 5, 4, 4],\n",
      "        [8, 4, 2, 2]])\n",
      "1 번째 Tensor 크기 torch.Size([2, 4])\n",
      "------------------------------\n",
      "2 번째 Tensor \n",
      "tensor([[9, 6, 3, 1],\n",
      "        [5, 6, 7, 3]])\n",
      "2 번째 Tensor 크기 torch.Size([2, 4])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randint(1, 10, (6, 4))  # (6,4) 텐서\n",
    "print(\"Original : \", tensor_a)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "chunk_num = 3\n",
    "chunk_tensor = torch.chunk(tensor_a, chunks = chunk_num, dim=0)  # dim = 0 (행), 6개의 행이 3개로 나누어 떨어지므로 3개의 텐서로 분리\n",
    "print(f'{len(chunk_tensor)} 개의 Tensor로 분리')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for idx,a in enumerate(chunk_tensor):\n",
    "    print(f'{idx} 번째 Tensor \\n{a}')\n",
    "    print(f'{idx} 번째 Tensor 크기', a.size())\n",
    "    print('---'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSrM4tYSv11K"
   },
   "source": [
    "#### 📝 설명 : 텐서 나누기\n",
    "* split : 입력한 **크기**로 여러 개의 작은 텐서로 나눕니다.\n",
    "  * split_size_or_sections 인자\n",
    "    * split_size (int): 얼마만큼의 크기로 자를 것인지\n",
    "    * sections (list): 얼마만큼의 크기로 **각각** 자를 것인지 (리스트 형태로 각 텐서의 크기를 각각 지정해 줄 수 있음)\n",
    "\n",
    "📚 참고할만한 자료:\n",
    "* [split] : https://pytorch.org/docs/stable/generated/torch.split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 759,
     "status": "ok",
     "timestamp": 1689147655702,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "1H2ons4twDBU",
    "outputId": "715b511a-3ffc-414c-e2ff-015b5f21cc9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 5, 4, 1],\n",
      "        [9, 4, 3, 1],\n",
      "        [5, 4, 4, 8],\n",
      "        [6, 7, 3, 2],\n",
      "        [7, 3, 6, 1],\n",
      "        [1, 4, 1, 1]])\n",
      "\n",
      "\n",
      "3 개의 Tensor로 분리\n",
      "\n",
      "\n",
      "0 번째 Tensor \n",
      "tensor([[3, 5, 4, 1],\n",
      "        [9, 4, 3, 1]])\n",
      "0 번째 Tensor 크기 torch.Size([2, 4])\n",
      "------------------------------\n",
      "1 번째 Tensor \n",
      "tensor([[5, 4, 4, 8],\n",
      "        [6, 7, 3, 2]])\n",
      "1 번째 Tensor 크기 torch.Size([2, 4])\n",
      "------------------------------\n",
      "2 번째 Tensor \n",
      "tensor([[7, 3, 6, 1],\n",
      "        [1, 4, 1, 1]])\n",
      "2 번째 Tensor 크기 torch.Size([2, 4])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randint(1, 10, (6, 4))  # (6,4) 텐서\n",
    "print(tensor_a)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "split_size = 2\n",
    "split_tensor = torch.split(tensor_a , split_size_or_sections = split_size, dim=0)  # dim = 0 (행), 텐서 A 를 행의 길이가 2 (split_size)인 텐서로 나눔\n",
    "print(f'{len(split_tensor)} 개의 Tensor로 분리')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for idx,a in enumerate(split_tensor):\n",
    "    print(f'{idx} 번째 Tensor \\n{a}')\n",
    "    print(f'{idx} 번째 Tensor 크기', a.size())\n",
    "    print('---'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1689147713081,
     "user": {
      "displayName": "Eddie(김윤기)",
      "userId": "11850705511374304262"
     },
     "user_tz": -540
    },
    "id": "gH_brWAqxsJa",
    "outputId": "5598d0b9-8ef4-4a06-de6d-1d42d1e6d41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :  tensor([[8, 5, 9, 5],\n",
      "        [9, 1, 9, 7],\n",
      "        [8, 1, 8, 3],\n",
      "        [8, 4, 7, 5],\n",
      "        [2, 6, 4, 8],\n",
      "        [1, 3, 4, 6]])\n",
      "\n",
      "\n",
      "2 개의 Tensor로 분리\n",
      "\n",
      "\n",
      "0 번째 Tensor \n",
      "tensor([[8, 5, 9, 5],\n",
      "        [9, 1, 9, 7]])\n",
      "0 번째 Tensor 크기 torch.Size([2, 4])\n",
      "------------------------------\n",
      "1 번째 Tensor \n",
      "tensor([[8, 1, 8, 3],\n",
      "        [8, 4, 7, 5],\n",
      "        [2, 6, 4, 8],\n",
      "        [1, 3, 4, 6]])\n",
      "1 번째 Tensor 크기 torch.Size([4, 4])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.randint(1, 10, (6, 4))  # (6,4) 텐서\n",
    "print(\"Original : \", tensor_a)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "split_num = [2, 4]\n",
    "split_tensor = torch.split(tensor_a, split_size_or_sections = split_num, dim=0)  # dim = 0 (행), 텐서 A 를 행의 길이가 (2개인 텐서와 4개인 텐서)로 나눔\n",
    "print(f'{len(split_tensor)} 개의 Tensor로 분리')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for idx,a in enumerate(split_tensor):\n",
    "    print(f'{idx} 번째 Tensor \\n{a}')\n",
    "    print(f'{idx} 번째 Tensor 크기', a.size())\n",
    "    print('---'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSVO_LtU2erF"
   },
   "source": [
    "#Reference\n",
    "> <b><font color = green>(📒가이드)\n",
    "- <a href='https://pytorch.org/docs/stable/index.html'>PyTorch 공식 문서</a>\n",
    "- <a href='https://inmoonlight.github.io/2021/03/03/PyTorch-view-transpose-reshape/'>view, transpose, reshape 비교</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4by_4X6tvaX6"
   },
   "source": [
    "## Required Package\n",
    "\n",
    "> torch == 2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P3G5QSQvbSg"
   },
   "source": [
    "## 콘텐츠 라이선스\n",
    "\n",
    "저작권 : <font color='blue'> <b> ©2023 by Upstage X fastcampus Co., Ltd. All rights reserved.</font></b>\n",
    "\n",
    "<font color='red'><b>WARNING</font> : 본 교육 콘텐츠의 지식재산권은 업스테이지 및 패스트캠퍼스에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1-whb-Pb5lc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
